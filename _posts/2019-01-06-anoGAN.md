---
layout: post
title: anoGAN(Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery)
tags: [Test, Markdown]
---

## Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery

![AnoGAN Paper](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2019-01-06-anoGAN/anogan_paper.png?raw=true)

이 논문은 2017년에 공개된 논문으로써 DCGAN을 적용하여 이상감지, 의료 영상에서 이상 영역 감지 등의 분야의 문제를 해결하는 방법을 소개한다.

---
### Abstract
모델들은 검출 자동화를 목표로 알려진 marker를 토대로 주석이 달려진 많은 양의 데이터를 기반으로 만들어진다.
Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection.

많은 Annotation의 노력과 알고있는 마커의 종류 한계는 이러한 접근 방법의 power를 제한한다.
High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches.

우리는 이미지 데이터의 이상을 마커의 후보로써 확인하기 위해 비교사 학습방법을 수행한다.
Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers.

우리는 DCGAN을 통해 정상 anatomical variability의 manifold를 학습하고, 이미지 공간에서 latent 공간으로의 맵핑을 기반으로한 anomaly scoring를 동반하는 AnoGAN을 제안한다.
We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space.


### 1. Introduction
많은 질병들은 충분한 데이터 셋이 부족한 반면, 다른 질병들은 마커의 예측력이 제한적이다.
many diseases lack a sufficiently broad set, while in others the predictive power of markers is limited.

또한, 예측가능한 마커를 알더라도, 영상 데이터에서 검출은 일반적으로 라벨이 되어있는 많은 양의 데이터를 토대로 광범위한 지도학습이 필요하다.
Furthermore, even if predictive markers are known, their computational detection in imaging data typically requires extensive supervised training using large amounts of annotated data such as labeled lesions.

이는 치료 결정을 위한 이미지 데이터를 활용하는 우리의 능력을 제한한다.
This limits our ability to exploit imaging data for treatment decisions.

우리는 건강한 국부 해부학적 외관의 풍부한 생성 모델을 생성하기 위해 비교사 학습을 제안한다.
Here, we propose unsupervised learning to create a rich generative model of healthy local anatomical appearance.

우리는 이미지 space에서 latent space로의 맵핑을을 위한 진보된 기법을 제안한다.
We propose an improved technique for mapping from image space to latent space.

우리는 학습 데이터를 따르는 관측치와 이에 적합하지 않는 데이터를 구분하기 위해 두 요소를 모두 사용한다.
We use both components to differentiate between observations that conform to the training data and such data that does not fit.


이 논문의 기여는, 우리는 비정상 이미지를 식별하고 이미지 데이터의 비정상 영역을 구분(Fig.2 red box)하기 위해 새로운 데이터를 평가할 수 있게하는 정상 모양의 생성 모델과 결합된 맵핑 스키마 대립 학습(Fig.1 blue box)을 제안한다.
Contribution In this paper, we propose adversarial training of a generative model of normal appearance and a coupled mapping schema that enables the evaluation of novel data to identify anomalous images and segment anomalous regions within imaging data.

![AnoGAN fig1](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2019-01-06-anoGAN/anogan_fig1.png?raw=true)

### 2. Generative Adversarial Representation Learning to Identify Anomalies
이상 감지를 위해, 우리는 GAN 기반의 정상 해부학적 가변성을 표현하는 모델을 학습한다.
To identify anomalies, we learn a model representing normal anatomical variability based on GANs.

이러한 방법은 생성모델과, 생성된 데이터와 실제 데이터를 동시에 구분하는 discriminator를 학습시킨다.
This method trains a generative model, and a discriminator to distinguish between generated and real data simultaneously.

단일 비용 함수 최적화 대신, 비용의 내쉬 균형, 생성 모델의 대표성과 특이성을 높이는 동시에 생성된 데이터로부터 실제 데이터를 더 정확하게 분류하고 대응하는 Feature Mapping을 개선하는 것을 목표로 한다.
Instead of a single cost function optimization, it aims at the Nash equilibrium of costs, increasing the representative power and specificity of the generative model, while at the same time becoming more accurate in classifying real from generated data and improving the corresponding feature mapping.

다음으로 우리는 어떻게 이 모델을 만들고, 이 모델을 사용해서 어떻게 학습 데이터에 나타나지 않은 형태를 식별하는지에 대해서 설명한다.
In the following we explain how to build this model, and how to use it to identify appearance not present in the training data.

![AnoGAN fig2](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2019-01-06-anoGAN/anogan_fig2.png?raw=true)
#### 2.1 Unsupervised Manifold Learning of Normal Anatomical variability
##### notation
$m = 1, 2, ..., M$이고, $I_m \in \mathbb{R}^{a \times b}$가 $a \times b$ 크기의 밝기 이미지일 때, 건강한 해부학 의료 이미지 $I_m$으로 이루어진 $M$개의 집합이 있다.
각 $I_m$ 이미지로부터 무작위로 샘플링된 위치에서 데이터 $\mathbf{x}=x_{k,m}\in \mathcal{X}$를 얻는다. 이때, $k=1,2,...,K$.
훈련 도중에 우리는 유일하게 $\langle I_m \rangle$만 제공되고, 학습 이미지의 가변성을 나타내는 manifold $\mathcal{X}$(Fig.2(b) blue region)를 배우기 위해 생성 대립 모델을 훈련시킨다.

테스트를 위해, 테스트 데이터 $\mathbf{J}$와 이진 라벨 Ground Truth $l_n \in \{0, 1\}$로부터 추출된 $c \times c$ 크기의 알지 못하는 이미지를 $\mathbf{y}_n$이라 할 때, $\langle \mathbf{y}_n, l_n \rangle$가 주어진다.
이러한 라벨은 주어진 병리학 기반의 이상 감지 성능 평가를 위한 테스트 중에만 주어진다.

<i><b>Encoding Anatomical Variability with a Generative Adversarial Network.</b></i>
GAN은 두개의 대립하는 모듈로 구성되어 있는데, 생성자 $G$와 구분자 $D$로 이루어져있다.
생성자 $G$ 는 latent space $\mathcal{Z}$로부터 샘플링된 입력 노이즈의 uniform 분포로 이루어진 1차원 벡터인 샘플 $\mathbf{z}$를 $G(\mathbf{z})$를 잘 알려진 건강한 예제로 구성된 이미지 space manifold $\mathcal{X}$의 2D 이미지로 맵핑하기 위해 데이터 $\mathbf{x}$를 따르는 분포 $p_g$를 배운다.

In this setting, the network architecture of the generator $G$ is equivalent to a convolutional decoder that utilizes a stack of strided convolutions.
The discriminator $D$ is a standard CNN that maps a 2D images to a single scalar value $D(\cdot)$.
The discriminator output $D(\cdot)$ can be interpreted as probability that the given input to the discriminator $D$ was a real image $\mathbf{x}$ sampled from training data $\mathcal{X}$ or generated $G(\mathbf{z})$ by the generator $G$.
$D$ and $G$ are simultaneously optimized through the following two-player minimax game with value function $V(G, D)$:
$\min$


#### 2.2 Mapping new Images to the Latent space
<b>Residual Loss</b>




#### 2.3 Detection of anomalies

### 3. Experiments


#### 3.1 Results
![AnoGAN fig3](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2019-01-06-anoGAN/anogan_fig3.png?raw=true)


![AnoGAN fig4](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2019-01-06-anoGAN/anogan_fig4.png?raw=true)

### 4. Conclusion



### 3. Focal loss
<i>Focal Loss</i>는 학습 중에서 전경과 배경이 극도로 imbalance(예, 1:1000)한 one-stage Object Detection 시나리오를 다루기 위해 설계되었다.

* cross entropy(CE) loss for binary classification
$$
CE(p, y)=\begin{cases}
-\log{(p)}, & \mathsf{if}\space y=1\\
-\log{(1-p)}, & \mathsf{otherwise.}
\end{cases}
$$

$y$는 Ground Truth(-1/1 또는 0/1)이고 $p$는 0~1 사이의 값을 지니며, $y=1$일 확률을 모델로부터 추정한 값이다.

$$
p_t=\begin{cases}
p, & \mathsf{if}\space y=1\\
1-p, & \mathsf{otherwise.}
\end{cases}
$$

$$CE(p,y)=CE(p_t)=-\log{(p_t)}$$

Fig.1을 보면, CE의 특징은 easily classified(잘 분류되는 샘플들, $p_t \gg .5$인 경우) 샘플들에 대해서도 Loss가 작지 않게 나타난다는 것이다. 즉, 잘 분류된 샘플들에 대해서도 loss가 꽤 크기 때문에 이들을 합치면 큰 값이 된다.
```
개인적인 소견
 확률이 높게 나타난 분류가 쉬운 샘플들도 loss가 크게 나타나기 때문에 이러한 경우에는 잘 분류된 샘플에 대해서도 에러를 줄이기 위한 학습이 많이 이루어 진다.
 즉 이 과정은 상대적으로 잘 분류된 샘플들 또한 모델이 학습하는데 주는 영향(가중치)가 크다고 생각할 수 있다.
 Focal Loss는 잘 분류된 샘플들에 대해서는(확률이 큰 샘플들) loss를 줄여 상대적으로 잘 분류가 안되는(또는 분류가 어려운) 샘플들에 대한 가중치를 상대적으로 높히는 형태로 디자인 되었다고 봄)
```

#### 3.1. Balanced Cross entropy
class imbalance 문제에 대한 일반적인 접근 방식은 class 1에 대해 가중치 $\alpha \in{[0, 1]}$를 적용하고 class -1에 대해 $1-\alpha$를 적용하는 것이다. $\alpha$ 결정은 반대 class의 빈도나 또는 corss validation을 통한 hyperparameter로써 결정한다. notation의 편의를 위해, $P_t$를 정의할 때 처럼 $\alpha_t$를 정의한다.
$\alpha$-balanced CE loss를 아래와 같이 작성한다.
$$
CE(p_t)=-\alpha_t \log{(P_t)}.
$$

#### 3.2. Focal Loss Definition
large class imbalance는 cross entropy loss가 dense detectors 학습 과정을 압도하면서 발생한다. 쉽게 분류된 negative 샘플들은 loss의 대부분을 이루고 있으며, gradient의 대부분을 지배하고 있다.

$\alpha$ 가 positive/negative 샘플의 중요도에 대해 균형을 잡기는 하지만, easy/hard 샘플에 대해서 구분하지는 않는다.
대신, 우리는 쉬운 샘플들에 대해서 가중치를 낮추는 형태로 loss function의 형태를 가공함으로써 어려운 샘플들에 대해서 집중하여 학습하는 방법을 제안한다.

우리는 tunable한 <i>focusing</i> 파라미터 $\gamma \geq 0$와 함께 조절 factor $(1-p_t)^\lambda$를 cross entropy loss에 추가하는 방법을 제안한다. focal loss는 다음과 같다.

$$
FL(p_t)=-(1-p_t)^\gamma \log{(p_t)}.
$$


focal loss의 두가지 속성은 다음과 같다.
<b>(1)</b> 어떤 샘플이 분류가 잘못되고, $p_t$가 작을 때, 조정 factor는 1에 가까운 값을 갖으며 losssms 거의 영향이 없다. 반면에 $p_t \rightarrow 1$일때에는 조정 factor가 0에 가까워 지고 잘 분류된 샘플에 대한 loss는 가중치가 작아지게 된다.
<b>(2)</b> focusing 파라미터인 $\gamma$는 easy 샘플들에 대한 가중치를 낮추는 정도를 somooth하게 조정한다. $\gamma = 0$일 때에는 FL은 CE와 같으며, $\gamma$가 증가하면 modulating factor의 효과가 증가하는 것과 같다.

직관적으로, 조정 factor는 easy 샘플들의 loss에 대한 영향을 줄이게하고, 샘플이 낮은 loss를 받는 구간을 확장시킨다. 예를 들어, $\gamma=2$이고 $p_t=0.9$로 분류된 샘플은 CE일때와 비교하여 100배 낮은 loss를 갖게되며, 그리고 $p_t \approx0.968$인 경우에는 100배 낮은 Loss를 갖게 된다. 이것은 잘못 분류된 샘플들에 대한 교정의 중요성을 증가시키게 한다($p_t \leq.5$, $\gamma=2$인 경우 loss는 최대 4배 까지 작아진다.).

### 4. RetinaNet Detector

RetinaNet은 <i>backbone</i> 네트워크와 두개의 task-specific <i>subnetworks</i>로 구성된 통합된 하나의 네트워크이다. backbone 네트워크는 입력된 전체 이미지에 대해서 convolutional feature map을 계산하는 역할을 수행한다. 첫번째 subnet은 backbone의 결과에서 convolutional 하게 object classification을 수행하는 단계이며, 두번째 subnet은 convolutional하게 bounding box를 추정하는 역할을 수행한다.

![Focal Loss RetinaNet architecture](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-12-07-Focal-loss-for-dense-object-detection/retinanet_architecture.png?raw=true)

### 5. Experiments

<b>$\alpha$, $\gamma$에 따른 성능 변화와 OHEM(online hard examples mining)과 성능 비교</b>

![Focal Loss Experiments](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-12-07-Focal-loss-for-dense-object-detection/experiment_compair.png?raw=true)


<b>타 Object Detection 알고리즘과 RetinaNet의 성능 비교</b>
![Object Detector Comparison](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-12-07-Focal-loss-for-dense-object-detection/object_detector_compare.png?raw=true)
