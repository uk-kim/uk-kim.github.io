---
layout: post
title: Variational Auto Encoder의 이해
tags: [Test, Markdown]
---

2014년 Auto-Encoding Variational Bayes, 2014(paper)가 등장한지 벌써 5년이 되어갑니다. 많은 시간이 흘러갔지만 아직도 수많은 논문과 연구에서는 VAE에 사용된 개념과 목적함수 ELBO가 반복적으로 등장하고 있습니다. 이에 VAE가 어떤 알고리즘인지, 그리고 그 안에는 어떤 철학이 담겨져있는지 중요하다고 생각되는 부분에 대해 공부한 내용을 담아보겠습니다.

위의 논문의 흐름에 따라 정리한 것은 아니며, 수학적인 의미 해석과 전개 과정을 중심으로 다루겠습니다.

_ _ _
### AE / VAE의 개요
![AE arch](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-10-06-vae/VAE_ELBO_kimsu_image_003.jpeg?raw=true)

Auto Encoder의 구조는 Encoder와 Decoder 두 파트로 구성됩니다. 어떤 입력 $$X$$에 대한 Embedding을 통해 압축된 정보를 Latent Variables에 담고, 이 Latent Variables로부터 다시 자기 자신($$X$$)를 복호화 하는 알고리즘이라고 볼 수 있습니다. 주로 사용되는 분야로는 Encoder를 통해 정보를 압축하거나 Decoder를 통해 정보를 복원하는 등의 역할을 수행할 수 있으며, 특징 추출 또는 정보 압축분야에 사용되는 PCA와 수학적 유사한 의미를 지니게 됩니다(수학적인 의미는 생략). 그리고 노이즈가 포함된 입력에 대하여 노이즈를 제거하는 De-nosing 문제에 효과적으로 사용될 수 있습니다.

![AE arch expr1](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-10-06-vae/VAE_ELBO_kimsu_image_004.jpeg?raw=true)

한단계만 더 들여다 보겠습니다. 64*64 픽셀의 얼굴 이미지들이 있다고 하겠습니다. 4096차원의 입력 이미지는 Encoder를 통해 저차원의 Hidden 공간으로 압축됩니다. 예를 들자면 피부색, 성별, 눈, 코, 입, 귀라는 6가지 특징들로 압축되었다고 가정하겠습니다. (이렇게 의미있는 정보가 담긴 압축된 hidden 차원 공간을 Latent Space, 이 변수들을 Latent Variables라 합니다.)

Decoder의 과정은 다시 피부색, 성별, 눈, 코, 입, 귀 의 특징값에 해당하는 새로운 얼굴 이미지를 생성하게 됩니다. 예로써 얼굴이 검고, 남자이며, 눈이 작고, 코가 크고, 입이 작으며, 귀가 큰 사람의 얼굴을 상상한다고 보시면 되겠습니다.


![AE arch expr2](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-10-06-vae/VAE_ELBO_kimsu_image_005.jpeg?raw=true)

Latent Space에는 입력 데이터들에 대해서 어떤 특징들로써 정보를 함축하고 있습니다. 그렇다면 <b>“Latent Space로부터 데이터를 생성해 낼 수는 없을까?”</b>라는 점에서 VAE가 주목받습니다.


![VAE arch expr1](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-10-06-vae/VAE_ELBO_kimsu_image_006.jpeg?raw=true)

가지고 있는 데이터(X)들은 너무 많고 고차원입니다. 그래서 새로운 데이터를 만들기가 어렵습니다. 만약, 샘플(X)들의 분포를 사전에 알고있는 어떤 분포로 표현할 수 있다면, 이 분포 내에서 샘플링을 통해 새로운 데이터 X를 추정해낼 수 있을 것입니다. (실제 데이터의 분포와 알고있는 어떤 분포를 맵핑시켜서)

그래서 AutoEncoder의 Latent Space가 우리가 잘 아는 정규분포를 따른다면, 이 분포는 평균(mu)과 표준편차(sigma)만 구해낸다면 분포를 표현할 수 있습니다.

따라서 Encoder의 출력이 mu와 sigma가 된다면, 정규분포 N(mu, sigma)에서 샘플링을 통해 잠재 변수 z를 생성해낼수 있고, 생성된 잠재변수 z는 다시 Decoder를 통해 입력된 샘플 x의 분포로 맵핑되어 대응하는 새로운 데이터를 추정할 수 있습니다.


![VAE arch expr2](https://github.com/uk-kim/uk-kim.github.io/blob/master/_posts/2018-10-06-vae/VAE_ELBO_kimsu_image_007.jpeg?raw=true)

그런데 샘플링을 하게 되면 Random성을 띄기 때문에 학습을 할수 없게 됩니다. 그래서 논문에서는 Reparametrization trick이라는 트릭을 통해서 N(mu, sigma) + e, e~N(0,1)와 같이 e를 출력에 더해주는 형태를 취해 z 값에 encoder가 미치는 영향 관계를 찾을 수 있게 됩니다. 즉, 미분이 가능한 모델로 표현이 가능합니다.


- - -


First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell

{% highlight js %}
// count to ten
for (var i = 1; i <= 10; i++) {
    console.log(i);
}

// count to twenty
var j = 0;
while (j < 20) {
    j++;
    console.log(j);
}
{% endhighlight %}

Type on Strap uses KaTeX to display maths. Equations such as $$S_n = a \times \frac{1-r^n}{1-r}$$ can be displayed inline.

Alternatively, they can be shown on a new line:

$$ f(x) = \int \frac{2x^2+4x+6}{x-2} $$